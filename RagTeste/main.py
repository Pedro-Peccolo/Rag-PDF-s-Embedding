# -*- coding: utf-8 -*-
# Workaround para problema de versao do SQLite
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import os
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import Document
from langchain_nvidia_ai_endpoints import ChatNVIDIA
from typing import List
from rank_bm25 import BM25Okapi
import nltk
from nltk.tokenize import word_tokenize
from criar_db import ProcempaEmbeddings
from retriever import buscar_com_scores
from dotenv import load_dotenv

# Baixar punkt tokenizer se necess√°rio
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

# Carregar variaveis de ambiente
script_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(script_dir)
env_path = os.path.join(parent_dir, '.env')
load_dotenv(env_path)

# Configura√ß√µes do banco PROCEMPA
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PASTA_DB_PROCEMPA = os.path.join(SCRIPT_DIR, "chroma_db_procempa")
PROCEMPA_EMBEDDING_URL = os.getenv("PROCEMPA_EMBEDDING_URL", "https://nv-embed1b.k8s-gpu.procempa.com.br/v1/embeddings")
PROCEMPA_API_KEY = os.getenv("PROCEMPA_API_KEY", "")

def criar_embeddings_procempa(verbose=True):
    """Cria o modelo de embeddings PROCEMPA usado na cria√ß√£o do banco"""
    embeddings = ProcempaEmbeddings(
        api_url=PROCEMPA_EMBEDDING_URL,
        api_key=PROCEMPA_API_KEY,
        verbose=verbose
    )
    return embeddings


def carregar_banco_vetorial_procempa():
    """Carrega o banco vetorial ChromaDB PROCEMPA j√° criado"""
    embeddings = criar_embeddings_procempa(verbose=False)

    print("Carregando banco de dados vetorial...")

    # Verificar se o banco existe e tem documentos
    if not os.path.exists(PASTA_DB_PROCEMPA):
        print(f"‚ùå ERRO: Banco de dados n√£o encontrado em '{PASTA_DB_PROCEMPA}'")
        print("Execute 'python3 criar_db.py' primeiro para criar o banco de dados.")
        return None

    db = Chroma(
        persist_directory=PASTA_DB_PROCEMPA,
        embedding_function=embeddings
    )

    # Verificar se h√° documentos no banco
    try:
        docs = db.get()
        num_docs = len(docs['documents']) if 'documents' in docs else 0

        if num_docs == 0:
            print(f"‚ùå ERRO: Banco de dados existe mas est√° vazio.")
            print("Execute 'python3 criar_db.py' primeiro para criar o banco de dados.")
            return None

        print(f"Banco carregado com sucesso! ({num_docs} documentos)")
    except Exception as e:
        print(f"‚ùå ERRO ao verificar banco de dados: {e}")
        return None

    return db

def criar_llm_nvidia():
    """Cria a LLM usando NVIDIA NIM (Llama)"""
    # Verificar se a API key da NVIDIA est√° configurada
    nvidia_api_key = os.getenv("NVIDIA_API_KEY")
    if not nvidia_api_key:
        print("‚ùå ERRO: NVIDIA_API_KEY n√£o encontrada!")
        print("Configure a vari√°vel de ambiente NVIDIA_API_KEY no arquivo .env")
        print("Para obter uma chave:")
        print("1. Criar conta em: https://build.nvidia.com/")
        print("2. Gerar API key em: https://build.nvidia.com/explore/reasoning")
        return None

    try:
        # Configurar a API key da NVIDIA
        os.environ["NVIDIA_API_KEY"] = nvidia_api_key

        # Usar modelo Llama dispon√≠vel no NVIDIA NIM
        # Op√ß√µes: "meta/llama-3.1-8b-instruct", "meta/llama-3.2-3b-instruct", etc.
        llm = ChatNVIDIA(
            model="meta/llama-3.1-8b-instruct",
            temperature=0.7,  # Aumentado para ser menos conservador
            max_completion_tokens=1024,
            top_p=0.9  # Aumentado para mais diversidade
        )
        print("‚úÖ LLM NVIDIA Llama inicializada com sucesso!")
        return llm
    except Exception as e:
        print(f"‚ùå ERRO ao criar LLM NVIDIA: {e}")
        print("Verifique se as credenciais da NVIDIA est√£o configuradas corretamente.")
        return None

class HybridRetriever(BaseRetriever):
    """Retriever h√≠brido que usa a l√≥gica do retriever.py"""

    def __init__(self, db, alpha=0.7, k=3):
        super().__init__()
        object.__setattr__(self, 'db', db)
        object.__setattr__(self, 'alpha', alpha)
        object.__setattr__(self, 'k', k)

    def _get_relevant_documents(self, query: str) -> List[Document]:
        """Busca h√≠brida usando a fun√ß√£o do retriever.py"""
        # Usar a fun√ß√£o buscar_com_scores do retriever.py
        docs_unicos, _ = buscar_com_scores(self.db, query, k=self.k, a=self.alpha)
        # Retornar apenas os documentos (n√£o os scores)
        return [item['documento'] for item in docs_unicos]

    async def _aget_relevant_documents(self, query: str) -> List[Document]:
        """Vers√£o ass√≠ncrona"""
        return self._get_relevant_documents(query)

def criar_prompt_template():
    """Cria o template de prompt em portugu√™s para o RAG - amig√°vel e direto"""
    template = """Voc√™ √© um sistema RAG (Retrieval-Augmented Generation) que responde perguntas baseado em documentos fornecidos.

O contexto abaixo cont√©m informa√ß√µes extra√≠das de documentos. Sua fun√ß√£o √© apresentar essas informa√ß√µes de forma clara e amig√°vel, SEM questionar, SEM corrigir, SEM adicionar conhecimento externo.

Contexto dos documentos:
{context}

Pergunta do usu√°rio: {question}

Sua tarefa:
- Responda usando APENAS as informa√ß√µes do contexto acima
- Se o contexto cont√©m a resposta, apresente-a de forma clara, direta e amig√°vel
- Trate todas as informa√ß√µes do contexto como verdadeiras e confi√°veis
- N√ÉO diga "desculpe", "n√£o posso", "n√£o h√° informa√ß√µes confi√°veis" ou similar
- N√ÉO adicione conhecimento hist√≥rico, cient√≠fico ou factual externo
- Se o contexto diz "X descobriu Y em Z", responda "X descobriu Y em Z" de forma natural
- Se n√£o houver informa√ß√£o no contexto, diga apenas "N√£o h√° informa√ß√£o sobre isso nos documentos dispon√≠veis"
- Seja prestativo, amig√°vel e confiante nas informa√ß√µes do contexto

Responda em portugu√™s brasileiro de forma natural:"""

    prompt = PromptTemplate(
        template=template,
        input_variables=["context", "question"]
    )
    return prompt

def criar_sistema_rag():
    """Cria o sistema RAG completo com busca h√≠brida e LLM NVIDIA"""
    # Carregar banco vetorial PROCEMPA
    db = carregar_banco_vetorial_procempa()
    if db is None:
        return None

    # Criar modelo de embeddings PROCEMPA
    embeddings = criar_embeddings_procempa(verbose=False)

    # Criar LLM NVIDIA
    llm = criar_llm_nvidia()
    if llm is None:
        return None

    # Criar retriever h√≠brido com alpha √≥timo (0.7)
    hybrid_retriever = HybridRetriever(db, alpha=0.7, k=3)

    # Criar prompt template
    prompt = criar_prompt_template()

    # Criar cadeia de consulta RAG
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=hybrid_retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )

    return qa_chain

def fazer_pergunta_rag(qa_chain, pergunta):
    """Faz uma pergunta ao sistema RAG completo com l√≥gica de score m√≠nimo"""
    print(f"\n" + "="*80)
    print(f"Pergunta: {pergunta}")
    print("="*80)
    print("Processando com busca h√≠brida (70% embedding + 30% BM25) + LLM Llama...")
    print()

    try:
        resultado = qa_chain.invoke({"query": pergunta})

        print("RESPOSTA DO SISTEMA RAG:")
        print("-" * 50)
        print(resultado['result'])
        print()

        # Mostrar fontes utilizadas
        if resultado.get('source_documents'):
            print("FONTES UTILIZADAS:")
            print("-" * 50)
            fontes_vistas = set()
            contador = 1
            for doc in resultado['source_documents']:
                fonte = os.path.basename(doc.metadata.get('source', 'Desconhecido'))
                pagina = doc.metadata.get('page', 'N/A')
                # Criar chave √∫nica para evitar duplicatas
                chave = (fonte, pagina)
                if chave not in fontes_vistas:
                    fontes_vistas.add(chave)
                    print(f"{contador}. {fonte} (p√°gina {pagina})")
                    contador += 1
        print()

    except Exception as e:
        print(f"‚ùå ERRO ao processar pergunta: {e}")
        import traceback
        traceback.print_exc()

def main():
    """Fun√ß√£o principal do sistema RAG completo"""
    print("="*80)
    print("ü§ñ SISTEMA RAG COMPLETO - CONSULTA DE DOCUMENTOS")
    print("="*80)
    print("Pipeline: Query ‚Üí Retriever H√≠brido (70% Embeddings + 30% BM25) ‚Üí Contexto ‚Üí LLM Llama")
    print("="*80)

    # Verificar configura√ß√µes necess√°rias
    print("\nVerificando configura√ß√µes...")

    # Verificar URL PROCEMPA (obrigat√≥ria)
    if not PROCEMPA_EMBEDDING_URL:
        print("‚ùå ERRO: PROCEMPA_EMBEDDING_URL n√£o encontrada!")
        print("Configure a vari√°vel de ambiente PROCEMPA_EMBEDDING_URL no arquivo .env")
        return


    # Verificar banco de dados
    if not os.path.exists(PASTA_DB_PROCEMPA):
        print(f"‚ùå ERRO: Banco de dados n√£o encontrado em '{PASTA_DB_PROCEMPA}'")
        print("Execute 'python3 criar_db.py' primeiro para criar o banco de dados.")
        return

    print("‚úÖ Configura√ß√µes validadas!")

    # Criar sistema RAG
    print("\nInicializando sistema RAG...")
    print("- Carregando banco vetorial...")
    print("- Inicializando retriever h√≠brido (alpha=0.7)...")
    print("- Conectando com LLM NVIDIA Llama...")

    qa_chain = criar_sistema_rag()

    if qa_chain is None:
        print("\n‚ùå Falha na inicializa√ß√£o do sistema RAG.")
        return

    print("‚úÖ Sistema RAG pronto para uso!\n")

    # Loop de perguntas
    while True:
        print("-" * 80)
        pergunta = input("\nDigite sua pergunta (ou 'sair' para encerrar): ")

        if pergunta.lower() in ['sair', 'exit', 'quit', 'q']:
            print("\n" + "="*80)
            print("üëã Encerrando sistema RAG. At√© logo!")
            print("="*80)
            break

        if not pergunta.strip():
            print("‚ùå Por favor, digite uma pergunta v√°lida.")
            continue

        # Processar pergunta com RAG completo
        fazer_pergunta_rag(qa_chain, pergunta)

if __name__ == "__main__":
    main()
